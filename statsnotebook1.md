 

Accelerated Testing
Acceptable Quality Level (AQL)
Analysis of Covariance (ANCOVA)
Asymmetric Price Transmission
Basic Local Alignment Search Tool  (BLAST)
Birnbaum–Saunders distribution
Bootstrap
Burr Distribution
Canonical Analysis
CHAID
Chow test
Circular distribution
Confounding
Continuity Correction
Copula Theory
Coverage probability.
Cox Regression
Cramér von Mises estimator
Cronbach's alpha
Davis distribution
Discriminant Analysis
Erlang Distribution
Fault Tree Analysis
Finite Population Correction
Generalized ESD test
Genetic Algorithm
Geostatistics
Gini coefficient
Gompertz Function
Hazard Function Estimation in Survival Analysis
Heckman Correction
Hidden Markov Models
Hills Criteria of Causation
Horn's Test of Principal Components
Influence Function
Iterative Bias Reduction
Jeffreys interval
Johnson Distributions
Kaplan Meier Estimation
Kappa statistics
Kernel Density Estimation
Kriging
Kruskal-Wallis Test
Latent variable models
Levy Distribution
Likelihood ratios
Log Rank Test
McNemar's Test
Markov random field
The Mills ratio
Modified PERT
Negative Predictive Value
Omitted-variable bias
Ordinal Dominance Statistics
Paralogistic Distribution
Pareto Distribution
Pareto Principal
Poisson Regression
Population Attributable Risk
Principal Component Analysis
Process Capability Index
Process Window Index
Rayleigh Distribution
Receiver Operating Characteristic
Renewal theory
Signal Detection Theory
Slash Distribution
Sortino Ratio
Trapezoidal Distribution
Violin Plot
Wilcoxon signed-rank test
Zipf distribution
Accelerated Testing
(industrial statistics)



Acceptable Quality Level (AQL)
(industrial statistics)
 
This is usually defined as the worst case quality level, in percentage or ratio, that is still considered acceptable. QA, (Quality Assurance), may be in charge of monitoring AQLs.

If a produced unit can have a number of different defects, then demerits can be assigned to each type of defect and product quality measured in terms of demerits. 

As an AQL is an acceptable level, the probability of acceptance for an AQL lot should be high. 


Analysis of Covariance (ANCOVA)
Asymmetric Price Transmission
(quantitative finance)
 
Basic Local Alignment Search Tool  (BLAST)
Birnbaum–Saunders distribution
(distributions)(industrial statistics)
 
 
The Birnbaum–Saunders distribution (also known as the fatigue life distribution) is a probability distribution used extensively in reliability applications to model failure times.
Bootstrap
Burr Distribution
(distributions)
 
The Burr Type XII distribution or simply the Burr distribution is a continuous probability distribution for a non-negative random variable.
 
The Burr is frequently used to model insurance claim sizes and household income, and is sometimes considered as an alternative to a Normal distribution when data show slight positive skewness.
 
It is also known as the Singh-Maddala distribution and is one of a number of different distributions sometimes called the "generalized log-logistic distribution".
Canonical Analysis
 
CHAID
CHAID is a type of decision tree technique, based upon adjusted significance testing (Bonferroni testing).

Chow test 
(econometrics)
 
The Chow test is a statistical and econometric test of whether the coefficients in two linear regressions on different data sets are equal.  
In econometrics, the Chow test is most commonly used in time series analysis to test for the presence of a structural break. 
In program evaluation, the Chow test is often used to determine whether the independent variables have different impacts on different subgroups of the population.
Circular distribution
(distributions)
 
A circular distribution or polar distribution is a probability distribution of a random variable whose values are angles, usually taken to be in the range [ 0,  2π ) .
A circular distribution is often a continuous probability distribution, and hence has a probability density, but such distributions can also be discrete, in which case they are called circular lattice distributions.

Confounding
(design of experiments)
 
When the differences between the treatment and control groups other than the treatment produce differences in response that are not distinguishable from the effect of the treatment, those differences between the groups are said to be confounded with the effect of the treatment (if any).
 
For example, prominent statisticians questioned whether differences between individuals that led some to smoke and others not to (rather than the act of smoking itself) were responsible for the observed difference in the frequencies with which smokers and non-smokers contract various illnesses. If that were the case, those factors would be confounded with the effect of smoking.
 
Confounding is quite likely to affect observational studies and experiments that are not randomized. Confounding tends to be decreased by randomization.
 
Continuity Correction
(distributions)
 
In using the normal approximation to the binomial probability histogram, one can get more accurate answers by finding the area under the normal curve corresponding to half-integers, transformed to standard units.
 
This is clearest if we are seeking the chance of a particular number of successes. For example, suppose we seek to approximate the chance of 10 successes in 25 independent trials, each with probability p = 40% of success. The number of successes in this scenario has a binomial distribution with parameters n = 25 and p = 40%.
 
The expected number of successes is np = 10, and the standard error is np(1-p)=6= 2.45.
If we consider the area under the normal curve at the point 10 successes, transformed to standard units, we get zero: the area under a point is always zero. We get a better approximation by considering 10 successes to be the range from 9.5 to 10.5 "successes".
 
The only possible number of successes between 9.5 and 10.5 is 10, so this is exactly right for the binomial distribution. Because the normal curve is continuous and a binomial random variable is discrete, we need to "smear out" the binomial probability over an appropriate range.
 
The lower endpoint of the range, 9 1/2 successes, is (9.5 − 10)/2.45 = −0.20 standard units.
The upper endpoint of the range, 10 1/2 successes, is (10.5 − 10)/2.45 = +0.20 standard units.
 
The area under the normal curve between −0.20 and +0.20 is about 15.8%.
 
The true binomial probability is 2510(0.4)10(0.6)15= 16%.
 
In a similar way, if we seek the normal approximation to the probability that a binomial random variable is in the range from i successes to k successes, inclusive, we should find the area under the normal curve from i−1/2 to k+1/2 successes, transformed to standard units.
 
If we seek the probability of more than i successes and fewer than k successes, we should find the area under the normal curve corresponding to the range i+1/2 to k−1/2 successes, transformed to standard units.
 
If we seek the probability of more than i but no more than k successes, we should find the area under the normal curve corresponding to the range i+1/2 to k+1/2 successes, transformed to standard units.
 
If we seek the probability of at least i but fewer than k successes, we should find the area under the normal curve corresponding to the range i−1/2 to k−1/2 successes, transformed to standard units. Including or excluding the half-integer ranges at the ends of the interval in this manner is called the continuity correction.
 
 
Copula Theory
(quantitative finance)
 

Coverage probability.
The coverage probability of a procedure for making confidence intervals is the chance that the procedure produces an interval that covers the truth.
Cox Regression
(generalized linear models)

Cramér von Mises estimator
The Cramér von Mises estimator is a “minimum distance”estimator (MDE), yielding the parameter value of the assumed distribution that minimizes its distance from the empirical distribution.
 
Cronbach's alpha
Cronbach's alpha is a statistic for investigating the internal consistency of a questionnaire (Cronbach, 1951; Bland & Altman, 1997).
 
Cronbach's α (alpha) is a  commonly known medical statistic. It is commonly used as a measure of the internal consistency reliability of a psychometric instrument. It was first named as alpha by Lee Cronbach in 1951, as he had intended to continue with further instruments. Cronbach's α measures how well a set of variables or items measures a single, unidimensional latent construct.
 
Davis distribution
(distributions)
 
The Davis distributions are a family of continuous probability distributions.
It is named after Harold T. Davis (1892–1974), who is 1941 proposed this distribution to model income sizes.
It is a generalization of the Planck's law of radiation from statistical physics.
 
Directional statistics

Directional statistics is the subdiscipline of statistics that deals with directions (unit vectors in Rn), axes (lines through the origin in Rn) or rotations in Rn. 

More generally, directional statistics deals with observations on compact Riemannian manifolds.



Discriminant Analysis
Erlang Distribution
(distributions)
 
The Erlang distribution is a continuous probability distribution which was developed by A. K. Erlang to examine the number of telephone calls which might be made at the same time to the operators of the switching stations. This work on telephone traffic engineering has been expanded to consider waiting times in queueing systems in general.
 
The Erlang distribution is now used in the fields of stochastic processes and of biomathematics.

Error Correction Model 

An error correction model is a dynamical system with the characteristics that the deviation of the current state from its long-run relationship will be fed into its short-run dynamics.
An error correction model is not a model that corrects the error in another model.



Fault Tree Analysis
A block diagram that gives a graphical picture of all possible means in which a failure, (fault or defect), could be generated in a design, process, or product, including external failures.
Finite Population Correction 
Generalized ESD test
(hypothesis testing) 

The Generalized Extreme Studentized Deviate (ESD) procedure can detect multiple outliers in one step (Rosner, 1983). 
Genetic Algorithm
(computation)
 
A genetic algorithm (GA) is a search heuristic that mimics the process of natural evolution. This heuristic is routinely used to generate useful solutions to optimization and search problems.
Geostatistics
Geostatistics is the generic name for a family of techniques which which are used for mapping of surfaces from limited sample data and the estimation of values at unsampled locations. 
 
Geostatistical estimation is a two stage process:
 
i.     studying the gathered data to establish the predictability of values from place to place in the study area; this study results in a graph known as a semi-variogram which models the difference between a value at one location and the value at another location according to the distance and direction between them;
 
ii.     estimating values at those locations which have not been sampled. This process is known as 'kriging'.
 
The basic technique "ordinary kriging" uses a weighted average of neighbouring samples to estimate the 'unknown' value at a given location. Weights are optimized using the semi-variogram model, the location of the samples and all the relevant inter-relationships between known and unknown values.
The technique also provides a "standard error" which may be used to quantify confidence levels.

 
Gini coefficient  
(distributions)(economics)


The Gini coefficient is a measure of the inequality of a distribution, a value of 0 expressing total equality and a value of 1 maximal inequality. It has found application in the study of inequalities in disciplines as diverse as sociology, economics, health science, ecology, chemistry, engineering and agriculture. The Gini coefficient is commonly used as a measure of inequality of income or wealth. 
Gompertz Function
(distributions)(time series) 

A Gompertz curve or Gompertz function, named after Benjamin Gompertz, is a sigmoid function. It is a type of mathematical model for a time series, where growth is slowest at the start and end of a time period.
 
Hazard Function Estimation in Survival Analysis
Heckman Correction
 

Heterogeneity and Homogeneity
In statistics, homogeneity and its opposite, heterogeneity, arise in describing the properties of a dataset, or several datasets. They relate to the validity of the often convenient assumption that the statistical properties of any one part of an overall dataset are the same as any other part.
 
Hidden Markov Models
Hills Criteria of Causation
Hills Criteria of Causation outlines the minimal conditions needed to establish a causal relationship between two items.  These criteria were originally presented by Austin Bradford Hill (1897-1991), a British medical statistician, as a way of determining the causal link between a specific factor (e.g., cigarette smoking) and a disease (such as emphysema or lung cancer). 
 
Hill's Criteria form the basis of modern epidemiological research, which attempts to establish scientifically valid causal connections between potential disease agents and the many diseases that afflict humankind.  While the criteria established by Hill (and elaborated by others) were developed as a research tool in the medical sciences, they are equally applicable to sociology, anthropology and other social sciences, which attempt to establish causal relationships among social phenomena. 
 
Indeed, the principles set forth by Hill form the basis of evaluation used in all modern scientific research.  While it is quite easy to claim that agent "A" (e.g., smoking) causes disease "B" (lung cancer), it is quite another matter to establish a meaningful, statistically valid connection between the two phenomena.  It is just as necessary to ask if the claims made within the social and behavioral sciences live up to Hill's Criteria as it is to ask the question in epidemiology (which is also a social and behavioral science). 
 
While it is quite easy to claim that population growth causes poverty or that globalization causes underdevelopment in Third World countries, it is quite another thing to demonstrate scientifically that such causal relationships, in fact, exist.  Hill's Criteria simply provides an additional valuable measure by which to evaluate the many theories and explanations proposed within the social sciences.
 
Horn's Test of Principal Components
Influence Function
Perhaps the most useful analytical tool for assessing whether, and the degree to which,a statistic is “robust” in the sense that it bounds or limits the influence of arbitrary deviations* from the assumed model is the Influence Function (IF), defined below:


=lim0T(F) - T(F)

where

•F is the distribution that is the assumed source of the data sample
•T is a statistical functional, that is, a statistic defined by the distribution that is the (assumed) source of the data sample. 

    [ For example, the statistical functional for the mean is  ] 
•x  is a particular point of evaluation, and the points being evaulated are those that deviate from the assumed .
•x is the probability measure that puts mass 1 at the point x.

* The terms “arbitrary deviation” and “contamination” or “statistical contamination” are used synonymously to mean data points that come froma distribution other than that assumed by the statistical model.They are not necessarily related to issues of data quality per se.



Iterative Bias Reduction
Jeffreys interval
 
The 'Jeffreys interval' is the Bayesian credible interval obtained when using the non-informative Jeffreys prior for the binomial proportion p. The Jeffreys prior for this problem is a Beta distribution with parameters (1/2, 1/2). After observing x successes in n trials, the posterior distribution for p is a Beta distribution with parameters (x + 1/2, n – x + 1/2).
 
When x ≠0  and x ≠ n, the Jeffreys interval is taken to be the 100(1 – α)% equal-tailed posterior probability interval, i.e. the α / 2 and 1 – α / 2 quantiles of a Beta distribution with parameters (x + 1/2, n – x + 1/2). These quantiles need to be computed numerically.
 
In order to avoid the coverage probability tending to zero when p → 0 or 1, when x = 0 the upper limit is calculated as before but the lower limit is set to 0, and when x = n the lower limit is calculated as before but the upper limit is set to 1.[1]
 
 
 
Johnson Distributions
(distributions)
 
 


Kaplan Meier Estimation
Kappa statistics
Kappa, is widely used to measure interobserver variability, that is, how often 2 or more observers agree in their interpretations. Simple agreement, the proportion of agreements between yes and no is a poor measure of agreement because it does not correct for chance. Kappa is the preferred statistic because it accounts for chance.
 
Kernel Density Estimation
Kernel density estimation is a non-parametric way of estimating the probability density function of a random variable.
 
Kriging
 
Kruskal-Wallis Test
(non parametric procedures)


The Kruskal-Wallis test is a nonparametric test used to compare three or more samples. It is used to test the null hypothesis that all populations have identical distribution functions against the alternative hypothesis that at least two of the samples differ only with respect to location (median), if at all.
It is the analogue to the F-test used in analysis of variance. While analysis of variance tests depend on the assumption that all populations under comparison are normally distributed, the Kruskal-Wallis test places no such restriction on the comparison.
 
 
Latent variable models
A latent variable model is a statistical model that relates a set of variables (so-called manifest variables) to a set of latent variables.

It is assumed that 1) the responses on the indicators or manifest variables are the result of an individual's position on the latent variable(s), and 2) that the manifest variables have nothing in common after controlling for the latent variable (local independence).


Levy Distribution
(distributions)
 
Likelihood ratios
(medical stats)
 
In evidence-based medicine, likelihood ratios are used for assessing the value of performing a diagnostic test.
They use the sensitivity and specificity of the test to determine whether a test result usefully changes the probability
that a condition (such as a disease state) exists.
 
Two versions of the likelihood ratio exist, one for positive and one for negative test results.
Respectively, they are known as the likelihood ratio positive (LR+) and likelihood ratio negative (LR–).
 
LR- =1 -sensitivityspecificity        LR+ =sensitivity1 -specificity
 
 
Log Rank Test
McNemar's Test
Markov-switching multifractal
(financial econometrics)
 
In financial econometrics, the Markov-switching multifractal (MSM) is a model of asset returns that incorporates stochastic volatility components of heterogeneous durations.
MSM captures the outliers, log-memory-like volatility persistence and power variation of financial returns.
In currency and equity series, MSM compares favorably with standard volatility models such as GARCH(1,1) and FIGARCH both in- and out-of-sample.
MSM is used by practitioners in the financial industry to forecast volatility, compute value-at-risk, and price derivatives.
 
 

Markov random field
 
A Markov random field, Markov network or undirected graphical model is a set of variables having a Markov property described by an undirected graph. A Markov random field is similar to a Bayesian network in its representation of dependencies.
 
It can represent certain dependencies that a Bayesian network cannot (such as cyclic dependencies); on the other hand, it can't represent certain dependencies that a Bayesian network can (such as induced dependencies). The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model.
 
A Markov random field is used to model various low to mid level tasks in image processing and computer vision.
For example, MRFs are used for image restoration, image completion, segmentation, texture synthesis, super-resolution and stereo matching.
 
The Mills ratio
 
 
 
 
The Mills ratio is defined as
 
 m(x)1h(x)S(x)p (x) 
 	
where h(x) is the hazard function, S(x) is the survival function and P(x) is the probability function.
 
Modified PERT
Negative Predictive Value
(medical stats)
 
The negative predictive value (NPV) is a summary statistic used to describe the performance of a diagnostic testing procedure.
 
It is defined as the proportion of subjects with a negative test result who are correctly diagnosed. A high NPV means that when the test yields a negative result, it is uncommon that the result should have been positive.
 
In the familiar context of medical testing, a high NPV means that the test only rarely misclassifies a sick person as being healthy. Note that this says nothing about the tendency of the test to mistakenly classify a healthy person as being sick.
 
Omitted-variable bias
Omitted-variable bias (OVB) occurs when a model is created which incorrectly leaves out one or more important causal factors.
The 'bias' is created when the model compensates for the missing factor by over- or under-estimating one of the other factors.
 
More specifically, OVB is the bias that appears in the estimates of parameters in a regression analysis, when the assumed specification is incorrect, in that it omits an independent variable (possibly non-delineated) that should be in the model.
Ordinal Dominance Statistics
Paralogistic Distribution
(distributions)
 

Pareto Distribution
(distributions)
 
The Pareto distribution is sometimes called the Bradford distribution.
 
Pareto Principal

The 80/20 rule that is based on Vilfredo Pareto's research that states only a vital few, (20%), of causes will have a greater impact than the many, (80%), causes. Usually used with a Pareto Chart.

Examples:

80% of the problems come from 20% of the causes
80% of a company's profits comes from 20% of their customers

Phi coefficient

The phi coefficient (also referred to as the "mean square contingency coefficient" and denoted by  or r) is a measure of association for two binary variables introduced by Karl Pearson.
 
This measure is similar to the Pearson correlation coefficient in its interpretation. In fact, a Pearson correlation coefficient estimated for two binary variables will return the phi coefficient.
Poisson Regression
(generalized linear models)
 
 



Population Attributable Risk
(medical stats)
 
In epidemiology, the term population attributable risk (PAR) has been described as the reduction in incidence
that would be observed if the population were entirely unexposed, compared with its current (actual) exposure
pattern.
In this context, the comparison is to the existing pattern of exposure, not the absence of exposure.
 
Principal Component Analysis
(multivariate statistics)
 
Principal Component Analysis aims to reduce the dimensionality of the data, so that the overall structure may be more easily described by summarising the main correlated variables using a smaller number of orthogonal combinations of them both.
Process Capability Index
(industrial statistics)
 
Process Window Index
(industrial statistics)


Quantitative Trait Locus
(genomics)

Quantitative traits refer to phenotypes (characteristics) that vary in degree and can be attributed to polygenic effects, i.e., product of two or more genes, and their environment. Quantitative trait loci (QTLs) are stretches of DNA containing or linked to the genes that underlie a quantitative trait.
Rayleigh Distribution
(distributions)(engineering)


 
The Rayleigh distribution is used as a model for wind speed, e.g. describing the distribution of wind speed over the period of a year. 
 
Receiver Operating Characteristic
 
A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot of the sensitivity, or true positive rate, vs. false positive rate ("1 − specificity" or "1 − true negative rate"), for a binary classifier system as its discrimination threshold is varied.
 
The ROC can also be represented equivalently by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate).
 
Renewal theory
(stochastic processes)
 
Renewal theory is the branch of probability theory that generalizes Poisson processes for arbitrary holding times.
 
Applications include calculating the expected time for a monkey who is randomly tapping at a keyboard to type the word Macbeth and comparing the long-term benefits of different insurance policies.
 
 
Sampling Fluctuation
 
Sampling fluctuation refers to the extent to which a statistic takes on different values with different samples. That is, it refers to how much the statistic's value fluctuates from sample to sample.
 
A statistic whose value fluctuates greatly from sample to sample is highly subject to sampling fluctuation.
Signal Detection Theory
(decision theory)(engineering)


Signal detection theory is a means to quantify the ability to discern between signal and noise. According to the theory, there are a number of determiners of how a detecting system will detect a signal, and where its threshold levels will be.
 
The theory can explain how changing the threshold will affect the ability to discern, often exposing how adapted the system is to the task, purpose or goal at which it is aimed.
 

Slash Distribution
(distributions)
 
The slash distribution is the probability distribution of a standard normal variate divided by an independent standard uniform variate.
 
In other words, if the random variable Z has a normal distribution with zero mean and unit variance, the random variable U has a uniform distribution on [0,1] and Z and U are statistically independent, then the random variable X = Z / U has a slash distribution.
 
The slash distribution is an example of a ratio distribution.
 
Sortino Ratio
(quantitative finance)
 
Spatial Econometrics
Spatial Econometrics is the field where spatial analysis and econometrics intersect. In general, econometrics differs from other branches of statistics in focusing on theoretical models, whose parameters are estimated using regression analysis. Spatial econometrics is a refinement of this, where either the theoretical model involves interactions between different entities, or the data observations are not truly independent. Thus, models incorporating spatial autocorrelation or neighborhood effects can be estimated using spatial econometric methods. Such models are common in regional science, real estate economics, and education economics.
 
Statistical Tolerance Limits
  
Trapezoidal Distribution
(distributions)
 
Trimean
 
The trimean is computed by adding the 25th percentile plus twice the 50th percentile plus the 75th percentile and dividing by four. What follows is an example of how to compute the trimean. Suppose that the 25th, 50th, and 75th percentile are 51, 55, and 63 respectively. Therefore, the trimean is computed as:
 
 
Trimean=51 +(255) +634  = 56.25
The trimean is almost as resistant to extreme scores as the median
 
Trimmed Mean
A trimmed mean is calculated by discarding a certain percentage of the lowest and the highest scores and then computing the mean of the remaining scores. For example, a mean trimmed 50% is computed by discarding the lower and higher 25% of the scores and taking
the mean of the remaining scores.
A trimmed mean is obviously less susceptible to the effects of extreme scores than is the arithmetic mean. Trimmed means are often used in Olympic scoring to minimize the effects of extreme ratings possibly caused by biased judges.
Violin Plot
(graphical methods)
 
A violin plot is a combination of a boxplot and a kernel density plot. Specifically, it starts with a box plot. It then adds a rotated kernel density plot to each side of the box plot.
 
Wilcoxon signed-rank test
(non parametric inference)
 
The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used when comparing two related samples or repeated measurements on a single sample to assess whether their population means differ (i.e. it's a paired difference test).
 
It can be used as an alternative to the paired Student's t-test when the population cannot be assumed to be normally distributed or the data is on the ordinal scale.
 
Zipf distribution
(distributions)
 
The Zipf distribution, sometimes referred to as the zeta distribution, is a discrete distribution commonly used in linguistics, insurance, and the modelling of rare events
 
 
